{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Natural Language Processing with Disaster Tweets: A Report**\n\n## **1. Introduction**\n\n### **1.1 Problem Statement**\nThe goal of this project is to classify tweets to determine whether they pertain to a real disaster or not. This challenge is part of a Kaggle competition titled \"Natural Language Processing with Disaster Tweets,\" where participants are tasked with developing models to identify disaster-related tweets. This task serves as an excellent introduction to Natural Language Processing (NLP) and machine learning, particularly in dealing with textual data.\n\n### **1.2 Data Description**\nThe dataset provided comprises two CSV files: `train.csv` and `test.csv`. The `train.csv` file contains 7,613 tweets, each labeled as either 1 (indicating a disaster-related tweet) or 0 (indicating a non-disaster-related tweet). The `test.csv` file includes 3,263 tweets that require classification by the model.\n\nThe dataset features the following columns:\n- `id`: A unique identifier for each tweet.\n- `keyword`: A keyword extracted from the tweet, which may contain missing values.\n- `location`: The location from where the tweet was sent, which may also contain missing values.\n- `text`: The actual text of the tweet.\n- `target`: The label indicating whether the tweet is about a disaster (1) or not (0).\n\nThe challenge lies in developing a model that can accurately classify tweets in the `test.csv` file based on the training data provided.\n\n## **2. Exploratory Data Analysis (EDA)**\n\n### **2.1 Data Inspection**\nInitially, the dataset was loaded and its structure inspected. The `train.csv` file contains 7,613 rows and 5 columns, while the `test.csv` file has 3,263 rows and 4 columns, lacking the `target` column present in the training data.\n\nThe target distribution in the training dataset shows a slight imbalance, with more tweets not related to disasters (4,342) compared to those that are disaster-related (3,271). This distribution highlights the importance of addressing potential imbalances in the data during the modeling process.\n\n### **2.2 Data Cleaning**\nUpon inspecting the data, it was evident that there were missing values in the `keyword` and `location` columns. However, since the primary focus of this analysis is on the `text` column, these missing values were not imputed. Instead, the emphasis was placed on preprocessing the text data to ensure it was ready for modeling.\n\n### **2.3 Data Visualization**\nVisualizations of the most frequent keywords and locations were generated to gain a better understanding of the tweet content and context. The most common keywords included terms like \"fatalities,\" \"deluge,\" and \"armageddon,\" while the most frequent locations were \"USA,\" \"New York,\" and \"London.\" These visualizations provided insights into the types of disasters discussed and the geographical distribution of the tweets.\n\n## **3. Data Preprocessing**\n\n### **3.1 Text Preprocessing**\nText preprocessing is a crucial step in Natural Language Processing. The following steps were applied to the `text` column to prepare it for modeling:\n\n1. **Removing URLs**: Tweets often contain URLs, which are not relevant for the analysis and were thus removed.\n2. **Removing Punctuation**: Punctuation marks were removed as they do not contribute to the semantic meaning of the text.\n3. **Removing Stopwords**: Common stopwords such as \"and,\" \"the,\" etc., which do not carry significant meaning, were removed from the text.\n4. **Stemming**: Words were reduced to their root forms using the Snowball Stemmer to standardize the vocabulary.\n\nThis preprocessing step was essential to ensure that the text data was clean and standardized, making it more suitable for modeling.\n\n### **3.2 Balancing the Dataset**\nGiven the slight imbalance in the dataset, undersampling was applied to the majority class (target = 0) to balance the data. This step involved reducing the number of non-disaster-related tweets to match the number of disaster-related tweets, ensuring that the model would not be biased towards the majority class.\n\n### **3.3 Splitting the Data**\nThe dataset was then split into training and validation sets. This split was necessary to evaluate the model's performance during the training phase and to ensure that the model could generalize well to unseen data.\n\n### **3.4 Text Tokenization and Padding**\nTo prepare the text data for input into the neural network, the text was tokenized and the sequences were padded. Tokenization involved converting the text into sequences of integers, where each integer represented a word in the vocabulary. Padding was used to ensure that all sequences had the same length, which is a requirement for input into the neural network.\n\n## **4. Model Architecture**\n\n### **4.1 Model Selection**\nGiven the sequential nature of the text data, a Long Short-Term Memory (LSTM) model was chosen for this task. LSTM is a type of Recurrent Neural Network (RNN) that is well-suited for capturing dependencies in sequential data, making it a suitable choice for text classification tasks such as this one.\n\n### **4.2 Word Embedding**\nTo convert the text data into a format that the LSTM model could process, word embeddings were used. The Keras embedding layer was employed to convert the integer sequences into dense vectors of fixed size, providing a rich representation of the text that the LSTM could use to learn patterns in the data.\n\n### **4.3 Model Compilation**\nThe model was compiled using the Adam optimizer and a binary cross-entropy loss function. The Area Under the ROC Curve (AUC) metric was chosen to evaluate the model's performance, as it provides a robust measure of the model's ability to distinguish between the two classes.\n\n### **4.4 Model Training**\nThe model was trained for 10 epochs with a batch size of 64. During training, the model was fitted on the training data and its performance was validated on the validation data. This training process involved adjusting the model's weights to minimize the loss function, while monitoring the AUC metric to ensure that the model was learning to distinguish between disaster-related and non-disaster-related tweets.\n\n## **5. Results and Analysis**\n\n### **5.1 Model Performance**\nThe model's performance was evaluated based on the AUC and loss metrics on both the training and validation sets. While the model performed exceptionally well on the training data, achieving an AUC of 0.9993 by the final epoch, there was a noticeable drop in performance on the validation set, where the AUC stabilized around 0.8017. This discrepancy suggests that the model may be overfitting to the training data, capturing noise rather than generalizable patterns.\n\n### **5.2 Hyperparameter Tuning**\nTo address the issue of overfitting, several strategies could be explored, including:\n\n- **Adding Dropout Layers**: Dropout layers could be introduced to randomly deactivate neurons during training, thereby helping to prevent overfitting by making the model more robust.\n- **Adjusting Learning Rate**: Lowering the learning rate could help the model converge more slowly, potentially avoiding overfitting by allowing the model to learn more generalizable patterns.\n- **Using Regularization**: Techniques such as L2 regularization could be applied to penalize large weights, encouraging the model to generalize better to unseen data.\n\n### **5.3 Further Improvements**\nTo further improve the model's performance, additional enhancements could include:\n\n- **Using Pretrained Embeddings**: Incorporating pretrained embeddings such as GloVe or Word2Vec could provide the model with richer semantic information, improving its ability to understand the context and meaning of the tweets.\n- **Exploring Advanced Architectures**: More advanced architectures, such as bidirectional LSTMs or Gated Recurrent Units (GRUs), could be explored to capture more complex dependencies in the text data, potentially leading to better model performance.\n\n## **6. Conclusion**\n\n### **6.1 Summary**\nIn this project, an LSTM-based model was developed to classify tweets as disaster-related or not. While the model performed well on the training data, its performance on the validation data indicated potential overfitting. Various techniques, such as dropout, learning rate adjustments, and regularization, can be applied to improve the model's generalization ability.\n\n### **6.2 Future Work**\nFuture work could focus on experimenting with pretrained embeddings, exploring more sophisticated architectures, and applying extensive hyperparameter tuning to optimize the model's performance. Additionally, data augmentation techniques could be explored to address the slight imbalance in the dataset, further improving the model's robustness.\n\n### **6.3 Key Takeaways**\n- **NLP Challenges**: Text classification in NLP requires careful preprocessing and thoughtful model design to achieve good performance.\n- **Overfitting**: Overfitting is a significant challenge in machine learning, particularly with complex models like LSTMs, and requires strategies such as regularization and dropout to mitigate.\n- **Model Optimization**: Continuous experimentation with model architectures, embeddings, and hyperparameters is essential for improving model performance and achieving better results.\n\n## **7. References**\n\n- Kaggle Competition: [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)\n- TensorFlow Documentation: [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)\n- GloVe: [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n- Word2Vec: [Word2Vec on TensorFlow](https://www.tensorflow.org/tutorials/text/word2vec)\n\nThis report provides a detailed overview of the steps taken to build a model for classifying disaster-related tweets, offering insights into the challenges encountered and potential improvements in the field of Natural Language Processing.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-14T19:00:03.648299Z","iopub.execute_input":"2024-08-14T19:00:03.648688Z","iopub.status.idle":"2024-08-14T19:00:04.621420Z","shell.execute_reply.started":"2024-08-14T19:00:03.648658Z","shell.execute_reply":"2024-08-14T19:00:04.620328Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout,BatchNormalization\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport string\nimport re\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:00:14.736356Z","iopub.execute_input":"2024-08-14T19:00:14.736761Z","iopub.status.idle":"2024-08-14T19:00:29.593408Z","shell.execute_reply.started":"2024-08-14T19:00:14.736728Z","shell.execute_reply":"2024-08-14T19:00:29.592326Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-14 19:00:18.280469: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-14 19:00:18.280591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-14 19:00:18.422355: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"df_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndf_sample = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:34.856380Z","iopub.execute_input":"2024-08-14T19:02:34.856770Z","iopub.status.idle":"2024-08-14T19:02:34.898376Z","shell.execute_reply.started":"2024-08-14T19:02:34.856739Z","shell.execute_reply":"2024-08-14T19:02:34.897268Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df_data.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:38.324509Z","iopub.execute_input":"2024-08-14T19:02:38.324928Z","iopub.status.idle":"2024-08-14T19:02:38.333113Z","shell.execute_reply.started":"2024-08-14T19:02:38.324897Z","shell.execute_reply":"2024-08-14T19:02:38.331993Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(7613, 5)"},"metadata":{}}]},{"cell_type":"code","source":"df_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:41.998668Z","iopub.execute_input":"2024-08-14T19:02:41.999046Z","iopub.status.idle":"2024-08-14T19:02:42.021634Z","shell.execute_reply.started":"2024-08-14T19:02:41.999018Z","shell.execute_reply":"2024-08-14T19:02:42.020553Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_data.value_counts('keyword')","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:44.852875Z","iopub.execute_input":"2024-08-14T19:02:44.853276Z","iopub.status.idle":"2024-08-14T19:02:44.870270Z","shell.execute_reply.started":"2024-08-14T19:02:44.853246Z","shell.execute_reply":"2024-08-14T19:02:44.869065Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"keyword\nfatalities               45\ndeluge                   42\narmageddon               42\nsinking                  41\ndamage                   41\n                         ..\nforest%20fire            19\nepicentre                12\nthreat                   11\ninundation               10\nradiation%20emergency     9\nName: count, Length: 221, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_data.value_counts('location')","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:48.484726Z","iopub.execute_input":"2024-08-14T19:02:48.485128Z","iopub.status.idle":"2024-08-14T19:02:48.501415Z","shell.execute_reply.started":"2024-08-14T19:02:48.485099Z","shell.execute_reply":"2024-08-14T19:02:48.500032Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"location\nUSA               104\nNew York           71\nUnited States      50\nLondon             45\nCanada             29\n                 ... \nHueco Mundo         1\nHughes, AR          1\nHuntington, WV      1\nHuntley, IL         1\nåø\\_(?)_/åø         1\nName: count, Length: 3341, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_data.value_counts('target')","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:51.504680Z","iopub.execute_input":"2024-08-14T19:02:51.505087Z","iopub.status.idle":"2024-08-14T19:02:51.516336Z","shell.execute_reply.started":"2024-08-14T19:02:51.505056Z","shell.execute_reply":"2024-08-14T19:02:51.515232Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"target\n0    4342\n1    3271\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:54.384869Z","iopub.execute_input":"2024-08-14T19:02:54.385262Z","iopub.status.idle":"2024-08-14T19:02:54.395965Z","shell.execute_reply.started":"2024-08-14T19:02:54.385232Z","shell.execute_reply":"2024-08-14T19:02:54.394816Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class_0 = df_data[df_data['target'] == 0]\nclass_1 = df_data[df_data['target'] == 1]\nclass_0_under = class_0.sample(len(class_1), random_state=123)\ndf_data = pd.concat([class_0_under, class_1])[['id','text','target']]","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:57.519437Z","iopub.execute_input":"2024-08-14T19:02:57.519821Z","iopub.status.idle":"2024-08-14T19:02:57.538724Z","shell.execute_reply.started":"2024-08-14T19:02:57.519790Z","shell.execute_reply":"2024-08-14T19:02:57.537596Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_data = shuffle(df_data)\ndf_train, df_val = train_test_split(df_data, test_size=0.20)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:02:59.957908Z","iopub.execute_input":"2024-08-14T19:02:59.958333Z","iopub.status.idle":"2024-08-14T19:02:59.967861Z","shell.execute_reply.started":"2024-08-14T19:02:59.958282Z","shell.execute_reply":"2024-08-14T19:02:59.966699Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"text = df_train['text'].to_numpy()\nlabel = df_train['target'].to_numpy()\ntext_val = df_val['text'].to_numpy()\nlabel_val = df_val['target'].to_numpy()\ntext","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:03:02.844494Z","iopub.execute_input":"2024-08-14T19:03:02.844881Z","iopub.status.idle":"2024-08-14T19:03:02.852806Z","shell.execute_reply.started":"2024-08-14T19:03:02.844853Z","shell.execute_reply":"2024-08-14T19:03:02.851597Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array(['Lab today ready for these body bags. ??',\n       \"There's a weird siren going off here...I hope Hunterston isn't in the process of blowing itself to smithereens...\",\n       'i decided to take a break from my emotional destruction to watch tangled then watch desolation of smaug',\n       ...,\n       'Governor weighs parole for California school bus hijacker http://t.co/7NPBfRzEJL http://t.co/Y0kByy8nce',\n       '@Silent0siris why not even more awesome norse landscapes with loads of atmosphere and life than boring/dead snotgreen wastelands =/',\n       'Dakota Skye gets horny with some porn then gets her juicy pussy pounded http://t.co/qew4c5M1xd View and download video'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_sentence(sentence):\n    sentence = re.sub(r'http\\S+|www\\S+|https\\S+', '', sentence, flags=re.MULTILINE)\n    sentence = re.sub(r'#\\S+', '', sentence)\n    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n    words = sentence.split()\n    words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(words)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:03:15.415020Z","iopub.execute_input":"2024-08-14T19:03:15.415462Z","iopub.status.idle":"2024-08-14T19:03:15.422229Z","shell.execute_reply.started":"2024-08-14T19:03:15.415429Z","shell.execute_reply":"2024-08-14T19:03:15.421132Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"text = [preprocess_sentence(x) for x in text]\nprint(text[0])\ntext = [stemmer.stem(word) for word in text]\nprint(text[0])\nword_tokenizer = tf.keras.preprocessing.text.Tokenizer()\nword_tokenizer.fit_on_texts(text)\nvocab_length = len(word_tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:03:20.244841Z","iopub.execute_input":"2024-08-14T19:03:20.245236Z","iopub.status.idle":"2024-08-14T19:03:20.603778Z","shell.execute_reply.started":"2024-08-14T19:03:20.245205Z","shell.execute_reply":"2024-08-14T19:03:20.602842Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Lab today ready body bags\nlab today ready body bag\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_text(text):\n    print(text[0])\n    text = [preprocess_sentence(x) for x in text]\n    print(text[0])\n    text = [stemmer.stem(word) for word in text]\n    print(text[0])\n    sequences = word_tokenizer.texts_to_sequences(text)\n    \n    print(sequences[0])\n    padded_sequences = pad_sequences(sequences)\n    print(padded_sequences[0])\n    \n    return padded_sequences","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:03:25.384583Z","iopub.execute_input":"2024-08-14T19:03:25.385023Z","iopub.status.idle":"2024-08-14T19:03:25.391946Z","shell.execute_reply.started":"2024-08-14T19:03:25.384988Z","shell.execute_reply":"2024-08-14T19:03:25.390352Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"texts = preprocess_text(text)\nlabels = to_categorical(label)\ntexts_val = preprocess_text(text_val)\nlabels_val = to_categorical(label_val)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:03:28.544960Z","iopub.execute_input":"2024-08-14T19:03:28.545369Z","iopub.status.idle":"2024-08-14T19:03:28.910412Z","shell.execute_reply.started":"2024-08-14T19:03:28.545338Z","shell.execute_reply":"2024-08-14T19:03:28.909226Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"lab today ready body bag\nlab today ready body bag\nlab today ready body bag\n[341, 52, 805, 26, 132]\n[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0 341  52 805  26 132]\nI'll be at SFA very soon....#Pandemonium http://t.co/RW8b50xz9m\nIll SFA soon\nill sfa soon\n[407, 441]\n[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0 407 441]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_length, 128))\n# model.add(Dropout(0.5))\nmodel.add(LSTM(64))\nmodel.add(Dense(2, activation='sigmoid'))\n\nmodel.compile(loss='BinaryCrossentropy', optimizer='Adam', metrics=['AUC'])\n# model.summary()\nmodel.fit(texts, labels, validation_data=(texts_val,labels_val), epochs=10, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:03:32.662876Z","iopub.execute_input":"2024-08-14T19:03:32.663267Z","iopub.status.idle":"2024-08-14T19:03:59.533365Z","shell.execute_reply.started":"2024-08-14T19:03:32.663235Z","shell.execute_reply":"2024-08-14T19:03:59.532125Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - AUC: 0.6417 - loss: 0.6581 - val_AUC: 0.8361 - val_loss: 0.4960\nEpoch 2/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - AUC: 0.9391 - loss: 0.3321 - val_AUC: 0.8279 - val_loss: 0.5299\nEpoch 3/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - AUC: 0.9838 - loss: 0.1638 - val_AUC: 0.8202 - val_loss: 0.6221\nEpoch 4/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - AUC: 0.9933 - loss: 0.0958 - val_AUC: 0.8156 - val_loss: 0.7022\nEpoch 5/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - AUC: 0.9967 - loss: 0.0688 - val_AUC: 0.7994 - val_loss: 0.8715\nEpoch 6/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - AUC: 0.9975 - loss: 0.0577 - val_AUC: 0.8073 - val_loss: 0.7917\nEpoch 7/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - AUC: 0.9987 - loss: 0.0470 - val_AUC: 0.8060 - val_loss: 0.8543\nEpoch 8/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - AUC: 0.9991 - loss: 0.0427 - val_AUC: 0.8088 - val_loss: 0.8839\nEpoch 9/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - AUC: 0.9991 - loss: 0.0417 - val_AUC: 0.8095 - val_loss: 0.8681\nEpoch 10/10\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - AUC: 0.9993 - loss: 0.0386 - val_AUC: 0.8017 - val_loss: 1.0158\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7fab8e2f5e40>"},"metadata":{}}]},{"cell_type":"code","source":"text2 = df_test['text']\ntext2 = text2.to_numpy()\ntext2 = preprocess_text(text2)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:04:04.250817Z","iopub.execute_input":"2024-08-14T19:04:04.251235Z","iopub.status.idle":"2024-08-14T19:04:04.446744Z","shell.execute_reply.started":"2024-08-14T19:04:04.251204Z","shell.execute_reply":"2024-08-14T19:04:04.445746Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Just happened a terrible car crash\nhappened terrible car crash\nhappened terrible car crash\n[858, 2255, 40, 18]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0  858 2255   40   18]\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = model.predict(text2)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:04:26.650987Z","iopub.execute_input":"2024-08-14T19:04:26.651372Z","iopub.status.idle":"2024-08-14T19:04:27.592833Z","shell.execute_reply.started":"2024-08-14T19:04:26.651342Z","shell.execute_reply":"2024-08-14T19:04:27.591667Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"prob_class_1 = predictions[:, 1]\nbinary_predictions = [1 if p >= 0.5 else 0 for p in prob_class_1]\ndf = pd.DataFrame({'target': binary_predictions}, index=df_test['id'])\ndf.index.name = 'id'\ndf.to_csv('/kaggle/working/submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-14T19:04:32.797534Z","iopub.execute_input":"2024-08-14T19:04:32.798031Z","iopub.status.idle":"2024-08-14T19:04:32.816776Z","shell.execute_reply.started":"2024-08-14T19:04:32.797987Z","shell.execute_reply":"2024-08-14T19:04:32.815699Z"},"trusted":true},"execution_count":21,"outputs":[]}]}